{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1vmxz3J8TIWr4p0svOipcTa1sTG3Duga7","authorship_tag":"ABX9TyN7jwEM9J+z6PMKtOvs1JpI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import csv\n","!pip install nlpaug\n","\n","\n","stop_words = ['the', 'and', 'or', 'is', 'a', 'an', 'of', 'in', 'to', 'for', 'with', 'on',\"/\",\"~\",\"`\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"()\",\"(\",\")\",\"<\",\">\",\"'\",\":\",'\"']\n","import string\n"],"metadata":{"id":"yVxdfklQr-V3","executionInfo":{"status":"ok","timestamp":1702993107280,"user_tz":-420,"elapsed":17914,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fed7d1e1-6562-4f21-85e7-550d126b4529"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.11.17)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iki50HTRqUzJ","executionInfo":{"status":"ok","timestamp":1702993126607,"user_tz":-420,"elapsed":19332,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"9f3d3669-8af1-4434-dcf6-da7f90b022f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'poem-sentiment'...\n","remote: Enumerating objects: 22, done.\u001b[K\n","remote: Counting objects: 100% (22/22), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 22 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (22/22), 33.54 KiB | 4.19 MiB/s, done.\n","Resolving deltas: 100% (2/2), done.\n","Cloning into 'tweet_sentiment_extraction'...\n","remote: Enumerating objects: 15, done.\u001b[K\n","remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 15\u001b[K\n","Unpacking objects: 100% (15/15), 1.31 MiB | 7.28 MiB/s, done.\n","Downloading...\n","From: https://drive.google.com/uc?id=1nOq-QxagdrNYag51XDhQwTCTok6LN89h\n","To: /content/sentiment140.zip\n","100% 84.9M/84.9M [00:01<00:00, 52.8MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1XtKRlTEjPN-Yp65cZKuiEcGxzr4HCY8D\n","To: /content/MH_Campaign_Tweets_Sentiment_Scored_1723.csv\n","100% 735M/735M [00:10<00:00, 69.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1iEQeMmeOp0UQVSdbKI3JBw4TOWPQULy-\n","To: /content/tambahan_netral.txt\n","100% 249k/249k [00:00<00:00, 34.8MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1CsrcLUqX9kI4L_qG_iwtamVeCgAVQB1i\n","To: /content/tambahan_negatif.txt\n","100% 10.5k/10.5k [00:00<00:00, 23.0MB/s]\n"]}],"source":["#prepocess dataset\n","\n","#!git clone \"https://github.com/google-research-datasets/poem-sentiment/\"\n","!git clone \"https://huggingface.co/datasets/mteb/tweet_sentiment_extraction/\"\n","!gdown \"1nOq-QxagdrNYag51XDhQwTCTok6LN89h\" #sentimemt140\n","\n","!gdown \"1XtKRlTEjPN-Yp65cZKuiEcGxzr4HCY8D\" #yg lama\n","\n","!gdown \"1iEQeMmeOp0UQVSdbKI3JBw4TOWPQULy-\" #tambahan netral\n","\n","!gdown \"1CsrcLUqX9kI4L_qG_iwtamVeCgAVQB1i\" #tambahan negatif\n"]},{"cell_type":"code","source":["!pip install nlpaug\n","!unzip \"/content/sentiment140.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibE0hWIPEHG5","executionInfo":{"status":"ok","timestamp":1702993198472,"user_tz":-420,"elapsed":17064,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"3775a229-de4f-42a0-ce89-0348abbe2e1b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.11.17)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Archive:  /content/sentiment140.zip\n","  inflating: training.1600000.processed.noemoticon.csv  \n"]}]},{"cell_type":"code","source":["import csv\n","import re\n","import ast\n","import nlpaug.augmenter.word as aug\n","import string\n","import numpy as np\n","import random\n","import json\n","\n","stop_words = ['the', 'and', 'or', 'is', 'a', 'an', 'of', 'in', 'to', 'for', 'with', 'on',\"/\",\" /\",\" / \",\"/ \",\"~\",\"`\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"()\",\"(\",\")\",\"<\",\">\",\"'\",\":\",'\"','/',\"http\",\"sunny\",\"com\"]\n","path1 = \"/content/training.1600000.processed.noemoticon.csv\"\n","path2 = \"/content/MH_Campaign_Tweets_Sentiment_Scored_1723.csv\"\n","path3 = \"/content/tambahan_netral.txt\"\n","path4 = \"/content/tambahan_negatif.txt\"\n","path5 = \"/content/tweet_sentiment_extraction/train.jsonl\"\n","path6 = \"/content/tweet_sentiment_extraction/test.jsonl\"\n","\n","tulis_path = \"/content/drive/MyDrive/DATASET_CAPSTONE/full_AUG.csv\"\n","\n","kalimat_neg = []\n","score_neg = []\n","\n","kalimat_neu = []\n","score_neu = []\n","\n","kalimat_pos = []\n","score_pos = []\n","\n","jumlah = [0, 0, 0]\n","\n","# Preprocess the first dataset\n","with open(path1, \"r\", encoding=\"latin-1\") as data:\n","    file = csv.reader(data, delimiter=\",\", quotechar='\"')\n","    next(file)\n","    for j in file:\n","        try:\n","            lab = int(j[0])\n","            text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', j[5])\n","            text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","\n","            if lab == 0:\n","                score_neg.append([1, 0, 0])\n","                jumlah[0] += 1\n","                kalimat_neg.append(text)\n","            elif lab == 2:\n","                score_neu.append([0, 1, 0])\n","                jumlah[1] += 1\n","                kalimat_neu.append(text)\n","            elif lab == 4:\n","                score_pos.append([0, 0, 1])\n","                jumlah[2] += 1\n","                kalimat_pos.append(text)\n","        except Exception as e:\n","            print(\"error:\", e)\n","\n","print(\"First dataset processed:\", jumlah)\n","\n","# Preprocess the second dataset (MH SCORE SENTIMENT)\n","potong = 20000\n","count = 0\n","with open(path2, 'r') as file_csv:\n","    csv_reader = csv.reader(file_csv)\n","    next(csv_reader)\n","    for baris in csv_reader:\n","        text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', baris[6])\n","        text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","\n","        data = list(ast.literal_eval(baris[21]).values())\n","        label = data[:-1]\n","        l = np.argmax(label)\n","\n","        if label[l] > 0.95:\n","            if l == 0:\n","                score_neg.append([1, 0, 0])\n","                kalimat_neg.append(text)\n","                jumlah[0] += 1\n","            elif l == 1:\n","                score_neu.append([0, 1, 0])\n","                kalimat_neu.append(text)\n","                jumlah[1] += 1\n","            elif l == 2:\n","                score_pos.append([0, 0, 1])\n","                kalimat_pos.append(text)\n","                jumlah[2] += 1\n","            count += 1\n","        else:\n","            pass\n","        if count >= potong:\n","            break\n","\n","print(\"Second dataset processed:\", jumlah)\n","\n","# Additional neutral data\n","with open(path3, \"r\") as file:\n","    reader = file.readlines()\n","    for text in reader:\n","        text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', text)\n","        text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","        kalimat_neu.append(text)\n","        score_neu.append([0, 1, 0])\n","        jumlah[1] += 1\n","\n","print(\"Additional neutral data processed:\", jumlah)\n","\n","# Additional negative data\n","with open(path4, \"r\") as file:\n","    reader = file.readlines()\n","    for text in reader:\n","        text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', text)\n","        text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","        kalimat_neg.append(text)\n","        score_neg.append([1, 0, 0])\n","        jumlah[2] += 1\n","\n","print(\"Additional negative data processed:\", jumlah)\n","\n","# Preprocess tweet sentiment extraction dataset\n","for j in [path5, path6]:\n","    with open(j, \"r\") as data:\n","        for h in data:\n","            try:\n","                fil = json.loads(h)\n","\n","                lab = (fil[\"label\"])\n","                if lab == 0:\n","                    score_neg.append([1, 0, 0])\n","                    jumlah[0] += 1\n","                    text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', fil[\"text\"])\n","                    text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","                    kalimat_neg.append(text)\n","                elif lab == 1:\n","                    score_neu.append([0, 1, 0])\n","                    jumlah[1] += 1\n","                    text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', fil[\"text\"])\n","                    text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","                    kalimat_neu.append(text)\n","                elif lab == 2:\n","                    score_pos.append([0, 0, 1])\n","                    jumlah[2] += 1\n","                    text = re.sub(r'[-.,:;!?\\n@#]|//t', ' ', fil[\"text\"])\n","                    text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words])\n","                    kalimat_pos.append(text)\n","            except:\n","                print(\"error : \", h)\n","\n","print(\"Tweet sentiment extraction dataset processed:\", jumlah)\n","\n","# Augmentation\n","jumlah_kelas_terbesar = max(jumlah)\n","\n","for _ in range(jumlah_kelas_terbesar - len(kalimat_neg)):\n","    random_index = random.randint(0, len(kalimat_neg) - 1)\n","    original_text = kalimat_neg[random_index]\n","    augmented_text = aug.RandomWordAug().augment(original_text)\n","    kalimat_neg.append(augmented_text)\n","    score_neg.append([1, 0, 0])\n","    jumlah[0] += 1\n","\n","for _ in range(jumlah_kelas_terbesar - len(kalimat_neu)):\n","    random_index = random.randint(0, len(kalimat_neu) - 1)\n","    original_text = kalimat_neu[random_index]\n","    augmented_text = aug.RandomWordAug().augment(original_text)\n","    if isinstance(augmented_text, list):\n","        augmented_text = ' '.join(augmented_text)\n","    kalimat_neu.append(augmented_text)\n","    score_neu.append([0, 1, 0])\n","    jumlah[1] += 1\n","\n","for _ in range(jumlah_kelas_terbesar - len(kalimat_pos)):\n","    random_index = random.randint(0, len(kalimat_pos) - 1)\n","    original_text = kalimat_pos[random_index]\n","    augmented_text = aug.RandomWordAug().augment(original_text)\n","    kalimat_pos.append(augmented_text)\n","    score_pos.append([0, 0, 1])\n","    jumlah[2] += 1\n","\n","print(\"Augmentation results:\", jumlah)\n","\n","# Combine all data\n","full = []\n","for i in range(jumlah_kelas_terbesar):\n","    full.append([kalimat_neg[i], score_neg[i]])\n","    full.append([kalimat_neu[i], score_neu[i]])\n","    full.append([kalimat_pos[i], score_pos[i]])\n","\n","# Write data to CSV\n","with open(tulis_path, 'w', newline='', encoding='utf-8') as file:\n","    writer = csv.writer(file)\n","    for row in full:\n","        writer.writerow(row)\n","\n","print(\"Data written to:\", tulis_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Igk0lRZ7MTaC","executionInfo":{"status":"ok","timestamp":1702790599233,"user_tz":-420,"elapsed":124086,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"0d569cd5-6f09-4e86-bc0f-2c7708ff2739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First dataset processed: [799999, 0, 800000]\n","Second dataset processed: [799999, 20000, 800000]\n","Additional neutral data processed: [799999, 25458, 800000]\n","Additional negative data processed: [799999, 25458, 801058]\n","Tweet sentiment extraction dataset processed: [808781, 38006, 810743]\n","Augmentation results: [809685, 810743, 811801]\n","Data written to: /content/drive/MyDrive/DATASET_CAPSTONE/full_AUG.csv\n"]}]},{"cell_type":"code","source":["print(full[:10])\n","print(full[-10:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGRk2K1CMu_t","executionInfo":{"status":"ok","timestamp":1702790599234,"user_tz":-420,"elapsed":4,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"cd306d8f-309c-415f-8368-e95234d199af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[\"upset that he can't update his Facebook by texting it might cry as result School today also Blah\", [1, 0, 0]], [\"Today last day OCD Awareness Week Read about what OCD what it isn't OCDAwarenessWeek https co/BL0zfAYMsN OCDweek OCD mentalhealth SocialWorkTwitter socialwork\", [0, 1, 0]], ['I LOVE Health4UandPets u guys r best', [0, 0, 1]], ['Kenichan I dived many times ball Managed save 50 rest go out bounds', [1, 0, 0]], ['It was miraculous I immediately felt difference Hear how BrainsWay Deep TMS treatment helped Cathy reclaim her life from OCD after 30 years intensive treatments various medications https co/fx4YcUwuRx OCDWeek DeepTMS', [0, 1, 0]], ['im meeting up one my besties tonight Cant wait GIRL TALK', [0, 0, 1]], ['my whole body feels itchy like its fire', [1, 0, 0]], ['It IT DOES ğŸ˜¯ğŸ˜¯ğŸ˜¯ğŸ“šğŸ“– OCDAwarenessWeek ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™ My gosh it means SO much It took FIVE years from first word publication day Thank you saying that OCD OCDweek https co/p6rA8Eyeep', [0, 1, 0]], ['DaRealSunisaKim Thanks Twitter add Sunisa I got meet you once at HIN show here DC area you were sweetheart', [0, 0, 1]], [\"nationwideclass no it's not behaving at all i'm mad why am i here because I can't see you all over there\", [1, 0, 0]]]\n","[[[\"try harper ' island\"], [0, 0, 1]], [['PROUDBLOCKHEAD you did not win contest I enter how hectic school are right now'], [1, 0, 0]], ['still notlooking', [0, 1, 0]], [['go ahead'], [0, 0, 1]], [['at work eating muffins drinkin Arizona off'], [1, 0, 0]], ['freshly bread', [0, 1, 0]], [['Did you have little amp covered Yum you have munchies'], [0, 0, 1]], [['AbbeyMatibag you better rest off comp now'], [1, 0, 0]], ['/ /', [0, 1, 0]], [[\"explosivityy ' your placement\"], [0, 0, 1]]]\n"]}]},{"cell_type":"code","source":["!pip install googletrans==4.0.0-rc1\n","!pip install sentencepiece\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GM8rtKChppz","executionInfo":{"status":"ok","timestamp":1702997108209,"user_tz":-420,"elapsed":16862,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"f6871d9d-c6a0-4497-8ae1-072efb27e38b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.11.17)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.1.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}]},{"cell_type":"code","source":["from googletrans import Translator\n","\n","def terjemahkan_kalimat(kalimat, tujuan_bahasa='en'):\n","    translator = Translator()\n","    terjemahan = translator.translate(kalimat, dest=tujuan_bahasa)\n","    return terjemahan.text\n","\n","if __name__ == \"__main__\":\n","    kalimat_input = input(\"Masukkan kalimat: \")\n","    bahasa_tujuan = \"id\"\n","\n","    hasil_terjemahan = terjemahkan_kalimat(kalimat_input, bahasa_tujuan)\n","    print(f\"Terjemahan: {hasil_terjemahan}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQGt0Ikwhbpi","executionInfo":{"status":"ok","timestamp":1702997117703,"user_tz":-420,"elapsed":6408,"user":{"displayName":"Boy Boy","userId":"16460098163939158023"}},"outputId":"cc2a8f47-2adc-43b9-d421-cf1330611220"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Masukkan kalimat: helo\n","Terjemahan: Halo\n"]}]}]}